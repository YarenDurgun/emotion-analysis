{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lGn8lqEO6rM1"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "#nltk.download () # Download all -> press d, type all, type quit after\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import precision_score, accuracy_score, f1_score, confusion_matrix, recall_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/training.csv\"\n",
        "val_path = \"/content/validation.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "val_df = pd.read_csv(val_path)"
      ],
      "metadata": {
        "id": "wfhBZiBj9_Ac"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(column):\n",
        "    tokens = nltk.word_tokenize(column)\n",
        "    return [w for w in tokens if w.isalpha()]  "
      ],
      "metadata": {
        "id": "FreHSkdM-eE8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stop = stopwords.words('english')\n",
        "\n",
        "#train_df['without_stopwords'] = train_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "#val_df['without_stopwords'] = val_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ],
      "metadata": {
        "id": "YJvZgTZGGkgU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['tokenized'] = train_df.apply(lambda x: tokenize(x['text']), axis=1)\n",
        "val_df['tokenized'] = val_df.apply(lambda x: tokenize(x['text']), axis=1)"
      ],
      "metadata": {
        "id": "IourfoJ4AG_A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lmtzr = WordNetLemmatizer()\n",
        "train_df['lemmatized'] = train_df['tokenized'].apply(\n",
        "                    lambda lst:[lmtzr.lemmatize(word) for word in lst])\n",
        "val_df['lemmatized'] = val_df['tokenized'].apply(\n",
        "                   lambda lst:[lmtzr.lemmatize(word) for word in lst])\n",
        "\n",
        "print(train_df['lemmatized'])\n",
        "print(val_df['lemmatized'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouC8E6rhFDVx",
        "outputId": "9a1337af-aeec-4821-fcd2-71ad455645ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                             [i, didnt, feel, humiliated]\n",
            "1        [i, can, go, from, feeling, so, hopeless, to, ...\n",
            "2        [im, grabbing, a, minute, to, post, i, feel, g...\n",
            "3        [i, am, ever, feeling, nostalgic, about, the, ...\n",
            "4                                [i, am, feeling, grouchy]\n",
            "                               ...                        \n",
            "15995    [i, just, had, a, very, brief, time, in, the, ...\n",
            "15996    [i, am, now, turning, and, i, feel, pathetic, ...\n",
            "15997                [i, feel, strong, and, good, overall]\n",
            "15998    [i, feel, like, this, wa, such, a, rude, comme...\n",
            "15999    [i, know, a, lot, but, i, feel, so, stupid, be...\n",
            "Name: lemmatized, Length: 16000, dtype: object\n",
            "0       [im, feeling, quite, sad, and, sorry, for, mys...\n",
            "1       [i, feel, like, i, am, still, looking, at, a, ...\n",
            "2                   [i, feel, like, a, faithful, servant]\n",
            "3               [i, am, just, feeling, cranky, and, blue]\n",
            "4       [i, can, have, for, a, treat, or, if, i, am, f...\n",
            "                              ...                        \n",
            "1995    [im, having, ssa, examination, tomorrow, in, t...\n",
            "1996    [i, constantly, worry, about, their, fight, ag...\n",
            "1997    [i, feel, it, important, to, share, this, info...\n",
            "1998    [i, truly, feel, that, if, you, are, passionat...\n",
            "1999    [i, feel, like, i, just, wan, na, buy, any, cu...\n",
            "Name: lemmatized, Length: 2000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "uAblLA67bLeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "train_df['lemmatized'] = train_df.lemmatized.map(lambda l: [stemmer.stem(word) for word in l])\n",
        "train_df.lemmatized = train_df.lemmatized.str.join(sep=' ')\n",
        "\n",
        "val_df['lemmatized'] = val_df.lemmatized.map(lambda l: [stemmer.stem(word) for word in l])\n",
        "val_df.lemmatized = val_df.lemmatized.str.join(sep=' ')\n",
        "\n",
        "display(train_df[\"lemmatized\"])\n",
        "display(val_df[\"lemmatized\"])"
      ],
      "metadata": {
        "id": "rFFieocabM64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "3923126b-175c-4195-97b0-076a8ea3ce24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0                                      i didnt feel humili\n",
              "1        i can go from feel so hopeless to so damn hope...\n",
              "2              im grab a minut to post i feel greedi wrong\n",
              "3        i am ever feel nostalg about the fireplac i wi...\n",
              "4                                        i am feel grouchi\n",
              "                               ...                        \n",
              "15995    i just had a veri brief time in the beanbag an...\n",
              "15996    i am now turn and i feel pathet that i am stil...\n",
              "15997                        i feel strong and good overal\n",
              "15998    i feel like this wa such a rude comment and im...\n",
              "15999    i know a lot but i feel so stupid becaus i can...\n",
              "Name: lemmatized, Length: 16000, dtype: object"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0       im feel quit sad and sorri for myself but ill ...\n",
              "1       i feel like i am still look at a blank canva b...\n",
              "2                             i feel like a faith servant\n",
              "3                          i am just feel cranki and blue\n",
              "4           i can have for a treat or if i am feel festiv\n",
              "                              ...                        \n",
              "1995    im have ssa examin tomorrow in the morn im qui...\n",
              "1996    i constant worri about their fight against nat...\n",
              "1997    i feel it import to share this info for those ...\n",
              "1998    i truli feel that if you are passion enough ab...\n",
              "1999    i feel like i just wan na buy ani cute make up...\n",
              "Name: lemmatized, Length: 2000, dtype: object"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing..."
      ],
      "metadata": {
        "id": "KvcBsQj7aq8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words='english')\n",
        "train_ppd_df = cv.fit_transform(train_df[\"lemmatized\"])\n",
        "val_ppd_df = cv.transform(val_df[\"lemmatized\"])\n",
        "\n",
        "\n",
        "display(train_ppd_df)\n",
        "display(val_ppd_df)"
      ],
      "metadata": {
        "id": "gmNXwBhwYMtZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "388d12c9-8bda-4901-b612-52336e3e9e82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<16000x10082 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 136700 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<2000x10082 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 16253 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_labels = np.array(val_df['label'])\n",
        "train_labels = np.array(train_df['label'])\n",
        "\n",
        "#trainX,testX,trainY,testY = train_test_split(train_ppd_df,train_df.label)\n",
        "#display(testX)"
      ],
      "metadata": {
        "id": "uPJ4niqqc9tI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(train_ppd_df,train_labels)\n",
        "predictions_NB = mnb.predict(val_ppd_df)\n",
        "\n",
        "#mnb.fit(trainX,trainY)\n",
        "#predictions_NB = mnb.predict(testX)\n",
        "\n",
        "print(\"Accuracy Score -> \",accuracy_score(predictions_NB, val_labels)*100)\n",
        "confusion_matrix(y_true=val_labels, y_pred=predictions_NB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk3LcMqTcbk8",
        "outputId": "de61a559-6553-4744-c752-8ab4bcad3ebe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score ->  78.4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[512,  25,   0,   6,   6,   1],\n",
              "       [ 40, 647,   5,   8,   4,   0],\n",
              "       [ 29,  77,  68,   3,   1,   0],\n",
              "       [ 45,  33,   2, 188,   7,   0],\n",
              "       [ 35,  24,   0,   7, 144,   2],\n",
              "       [ 29,  32,   1,   1,   9,   9]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}