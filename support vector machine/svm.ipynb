{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "# nltk.download('all') # Download only required or all -> press d, type all, type quit after\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords # not using these stopwords, still may be useful in the future\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, confusion_matrix, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"content/training.csv\"\n",
    "val_path = \"content/validation.csv\"\n",
    "test_path = \"content/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokenized'] = train_df.apply(lambda x: tokenize(x['text']), axis=1)\n",
    "val_df['tokenized'] = val_df.apply(lambda x: tokenize(x['text']), axis=1)\n",
    "test_df['tokenized'] = test_df.apply(lambda x: tokenize(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                             [i, didnt, feel, humiliated]\n",
      "1        [i, can, go, from, feeling, so, hopeless, to, ...\n",
      "2        [im, grabbing, a, minute, to, post, i, feel, g...\n",
      "3        [i, am, ever, feeling, nostalgic, about, the, ...\n",
      "4                                [i, am, feeling, grouchy]\n",
      "                               ...                        \n",
      "15995    [i, just, had, a, very, brief, time, in, the, ...\n",
      "15996    [i, am, now, turning, and, i, feel, pathetic, ...\n",
      "15997                [i, feel, strong, and, good, overall]\n",
      "15998    [i, feel, like, this, wa, such, a, rude, comme...\n",
      "15999    [i, know, a, lot, but, i, feel, so, stupid, be...\n",
      "Name: lemmatized, Length: 16000, dtype: object\n",
      "0       [im, feeling, quite, sad, and, sorry, for, mys...\n",
      "1       [i, feel, like, i, am, still, looking, at, a, ...\n",
      "2                   [i, feel, like, a, faithful, servant]\n",
      "3               [i, am, just, feeling, cranky, and, blue]\n",
      "4       [i, can, have, for, a, treat, or, if, i, am, f...\n",
      "                              ...                        \n",
      "1995    [im, having, ssa, examination, tomorrow, in, t...\n",
      "1996    [i, constantly, worry, about, their, fight, ag...\n",
      "1997    [i, feel, it, important, to, share, this, info...\n",
      "1998    [i, truly, feel, that, if, you, are, passionat...\n",
      "1999    [i, feel, like, i, just, wan, na, buy, any, cu...\n",
      "Name: lemmatized, Length: 2000, dtype: object\n",
      "0       [im, feeling, rather, rotten, so, im, not, ver...\n",
      "1       [im, updating, my, blog, because, i, feel, shi...\n",
      "2       [i, never, make, her, separate, from, me, beca...\n",
      "3       [i, left, with, my, bouquet, of, red, and, yel...\n",
      "4       [i, wa, feeling, a, little, vain, when, i, did...\n",
      "                              ...                        \n",
      "1995    [i, just, keep, feeling, like, someone, is, be...\n",
      "1996    [im, feeling, a, little, cranky, negative, aft...\n",
      "1997    [i, feel, that, i, am, useful, to, my, people,...\n",
      "1998    [im, feeling, more, comfortable, with, derby, ...\n",
      "1999    [i, feel, all, weird, when, i, have, to, meet,...\n",
      "Name: lemmatized, Length: 2000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "train_df['lemmatized'] = train_df['tokenized'].apply(\n",
    "                    lambda lst:[lmtzr.lemmatize(word) for word in lst])\n",
    "val_df['lemmatized'] = val_df['tokenized'].apply(\n",
    "                   lambda lst:[lmtzr.lemmatize(word) for word in lst])\n",
    "test_df['lemmatized'] = test_df['tokenized'].apply(\n",
    "                   lambda lst:[lmtzr.lemmatize(word) for word in lst])\n",
    "\n",
    "print(train_df['lemmatized'])\n",
    "print(val_df['lemmatized'])\n",
    "print(test_df['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_df['label'])\n",
    "val_labels = np.array(val_df['label'])\n",
    "test_labels = np.array(test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "train_labels = Encoder.fit_transform(train_labels)\n",
    "val_labels = Encoder.fit_transform(val_labels)\n",
    "test_labels = Encoder.fit_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = ['a', 'an', 'the', 'and', 'is', 'are', 'am', 'for', 'in', 'of', 'at', 'on']\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000, stop_words=stop)\n",
    "train_X_Tfidf = Tfidf_vect.fit_transform(train_df['lemmatized'].apply(lambda x: ' '.join(x)))\n",
    "test_X_Tfidf = Tfidf_vect.transform(test_df['lemmatized'].apply(lambda x: ' '.join(x)))\n",
    "val_X_Tfidf = Tfidf_vect.transform(val_df['lemmatized'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=2**1)\n",
    "SVM.fit(train_X_Tfidf, train_labels)\n",
    "predictions = SVM.predict(test_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score ->  76.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[525,  48,   0,   5,   3,   0],\n",
       "       [ 11, 675,   7,   0,   1,   1],\n",
       "       [ 17,  97,  43,   2,   0,   0],\n",
       "       [ 46,  83,   0, 145,   1,   0],\n",
       "       [ 39,  56,   0,   5, 124,   0],\n",
       "       [ 10,  29,   0,   0,  14,  13]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy Score -> \", accuracy_score(predictions, test_labels)*100)\n",
    "confusion_matrix(y_true=test_labels, y_pred=predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2f3c33f8c72b8e7a278d6cb43d42205b2f3e4a0321aa0cf999c5968a7d8f812"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
